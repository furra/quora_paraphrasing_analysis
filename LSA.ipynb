{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#future work?\n",
    "contractions = {}\n",
    "with open('english_contractions.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        contraction, word = line.strip().split('|')\n",
    "        contractions[contraction] = word\n",
    "contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer methods\n",
    "\n",
    "TODO: add parameter for the different methods\n",
    "      add stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(stopwords.words('english'))\n",
    "def is_english(word):\n",
    "    #not quite, it accepts some latin characters\n",
    "    try:\n",
    "        word.encode('ISO-8859-1')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_number(word):\n",
    "    #plus hexadecimal numbers and 1 letter + numbers\n",
    "    return re.search(r'(^0x)|(^[0-9\\-\\.:xv]+$)|(^\\w\\d+$)', word)\n",
    "\n",
    "def text_from_number(word):\n",
    "    return re.search(r'^\\d+([a-z]+)$', word)\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    if not re.search(r'\\.com?\\b|\\.in\\b|\\.org\\b|\\.be\\b|\\.xyz\\b|\\.net\\b|\\.us\\b', word):\n",
    "        new_words = []\n",
    "        for term in word.split('/'):\n",
    "            characters = set(\"!#$%&'()*+^,\")\n",
    "            new_word = term.translate({ord(char) : None for char in characters})\n",
    "            delete_chars = \"+_\\-\\.\"\n",
    "            new_words.append(re.sub(r'^[{}]+|[{}]+$'.format(delete_chars, delete_chars), '', new_word))\n",
    "        return new_words\n",
    "    return [re.sub(r'/$', '', word)]\n",
    "\n",
    "def tokenize_question(doc):\n",
    "    words = [word for word in word_tokenize(doc) if word not in stoplist]\n",
    "    words = [token for word in words for token in remove_punctuation(word)] #remove some /\n",
    "    words = [word for word in words if len(word) > 1 and not is_number(word) and is_english(word)]\n",
    "    words = [text_from_number(word).groups()[0] if text_from_number(word) else word for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading questions and building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run Preprocess notebook first!\n",
    "question_pairs = pickle.load(open('data/question_pairs.list.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "#doc2qid = {} #question to question id, to correct question ids (check corrected data file)\n",
    "vocabulary = set()\n",
    "qt_filename = 'data/questions_tokenized.txt'\n",
    "\n",
    "with open(qt_filename, 'w') as qt_file:\n",
    "    for pair in question_pairs:\n",
    "        question1, question2 = pair[3:5]\n",
    "\n",
    "        token_q1 = tokenize_question(question1)\n",
    "        token_q2 = tokenize_question(question2)\n",
    "\n",
    "        qt_file.write('{}\\n{}\\n{}\\n{}\\n'.format(question1, ' '.join(token_doc1),\n",
    "                                                question2, ' '.join(token_doc2)))\n",
    "\n",
    "        if doc1 not in doc2qid:\n",
    "            doc2qid[doc1] = qid1\n",
    "            vocabulary.update(token_doc1)\n",
    "            if doc1:\n",
    "                documents.append(token_doc1)\n",
    "\n",
    "        if doc2 not in doc2qid:\n",
    "            doc2qid[doc2] = qid2\n",
    "            vocabulary.update(token_doc2)\n",
    "            if doc2:\n",
    "                documents.append(token_doc2)\n",
    "\n",
    "pkl_file = open('data/lsa_vocabulary.set.pkl', 'wb')\n",
    "pickle.dump(vocabulary, pkl_file)\n",
    "pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocabulary.txt','w') as outfile:\n",
    "    for word in vocabulary:\n",
    "        outfile.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vocabulary)\n",
    "print(len(documents))\n",
    "print('documents:', len(documents))\n",
    "print('vocabulary:', len(vocabulary),'\\n', vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(documents)\n",
    "dictionary.save('data/questions.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [dictionary.doc2bow(document) for document in documents]\n",
    "gensim.corpora.MmCorpus.serialize('data/questions.mm', vectors)\n",
    "print(vectors[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"how can I stay motivated to learn a new language\"\n",
    "new_vec = dictionary.doc2bow(new_doc.split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files\n",
    "\n",
    "If saved files previously, they can be read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('data/questions.dict')\n",
    "corpus = gensim.corpora.MmCorpus('data/questions.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.save('data/questions.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "lsa.save('data/questions.10d.lsa')\n",
    "lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
    "lsa.save('data/questions.100d.lsa')\n",
    "lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
    "lsa.save('data/questions.200d.lsa')\n",
    "lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "lsa.save('data/questions.300d.lsa')\n",
    "#corpus_lsa = lsa[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel.load('data/questions.tfidf')\n",
    "lsa = gensim.models.LsiModel.load('data/questions.10d.lsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs = pickle.load(open('data/question_pairs.pkl','rb'))\n",
    "lsa10 = gensim.models.LsiModel.load('data/questions.10d.lsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"what is npt? how would signing the npt affect india?\"\n",
    "doc2 = \"what is npt?\"\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "token_doc1 = tokenize_question(doc1)\n",
    "token_doc2 = tokenize_question(doc2)\n",
    "print(token_doc1)\n",
    "print(token_doc2)\n",
    "doc1_vec = lsa10[dictionary.doc2bow(token_doc1)]\n",
    "doc2_vec = lsa10[dictionary.doc2bow(token_doc2)]\n",
    "print(doc1_vec)\n",
    "print(doc2_vec)\n",
    "print(gensim.matutils.cossim(doc1_vec, doc2_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using cosine similarity a file is generated to be evaluated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "#just to debug time\n",
    "#TODO: check magic commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial implementation (kinda slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ndim in [10, 100, 200, 300]:\n",
    "    print('Evaluating {} dimensions'.format(ndim))\n",
    "    filename = 'data/lsa_similarities_{}d.txt'.format(ndim)\n",
    "    print('{} - Loading model...'.format(strftime('%H:%M:%S')))\n",
    "    lsa = gensim.models.LsiModel.load('data/questions.{}d.lsa'.format(ndim))\n",
    "    outfile = open(filename, 'w')\n",
    "    errfile = None\n",
    "    print('{} - Starting evaluation...'.format(strftime('%H:%M:%S')))\n",
    "    \n",
    "    for _, pair in enumerate(question_pairs):\n",
    "        pair_id, qid1, qid2, doc1, doc2, label = pair\n",
    "        token_doc1 = tokenize_question(doc1)\n",
    "        token_doc2 = tokenize_question(doc2)\n",
    "        \n",
    "        doc1_vec = lsa[dictionary.doc2bow(token_doc1)]\n",
    "        doc2_vec = lsa[dictionary.doc2bow(token_doc2)]\n",
    "        similarity = gensim.matutils.cossim(doc1_vec, doc2_vec)\n",
    "        \n",
    "        outfile.write('{}|{}|{}\\n'.format(pair_id, label, similarity))\n",
    "        \n",
    "    print('{} - Evaluation finished.'.format(strftime('%H:%M:%S')))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel implementation\n",
    "\n",
    "TODO: finish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "for ndim in [10, 100, 200, 300]:\n",
    "    print('Evaluating {} dimensions'.format(ndim))\n",
    "    filename = 'data/lsa_similarities_{}d.txt'.format(ndim)\n",
    "    print('{} - Loading model...'.format(strftime('%H:%M:%S')))\n",
    "    lsa = gensim.models.LsiModel.load('data/questions.{}d.lsa'.format(ndim))\n",
    "    outfile = open(filename, 'w')\n",
    "    errfile = None\n",
    "    print('{} - Starting evaluation...'.format(strftime('%H:%M:%S')))\n",
    "    \n",
    "    for pair in question_pairs:\n",
    "        pair_id, qid1, qid2, doc1, doc2, label = pair\n",
    "        token_doc1 = tokenize_question(doc1)\n",
    "        token_doc2 = tokenize_question(doc2)\n",
    "        \n",
    "        doc1_vec = lsa[dictionary.doc2bow(token_doc1)]\n",
    "        doc2_vec = lsa[dictionary.doc2bow(token_doc2)]\n",
    "        similarity = gensim.matutils.cossim(doc1_vec, doc2_vec)\n",
    "\n",
    "    outfile.write('{}|{}|{}\\n'.format(pair_id, label, similarity))\n",
    "\n",
    "    print('{} - Evaluation finished.'.format(strftime('%H:%M:%S')))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
