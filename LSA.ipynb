{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import casual_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#future work?\n",
    "contractions = {}\n",
    "with open('english_contractions.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        contraction, word = line.strip().split('|')\n",
    "        contractions[contraction] = word\n",
    "contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer methods\n",
    "\n",
    "TODO: add parameter for the different methods\n",
    "      add stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(stopwords.words('english'))\n",
    "def is_english(word):\n",
    "    #not quite, it accepts some latin characters\n",
    "    try:\n",
    "        word.encode('ISO-8859-1')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_number(word):\n",
    "    #plus hexadecimal numbers and 1 letter + numbers\n",
    "    return re.search(r'(^0x)|(^[0-9\\-\\.:xv]+$)|(^\\w\\d+$)', word)\n",
    "\n",
    "def text_from_number(word):\n",
    "    return re.search(r'^\\d+([a-z]+)$', word)\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    if not re.search(r'\\.com?\\b|\\.in\\b|\\.org\\b|\\.be\\b|\\.xyz\\b|\\.net\\b|\\.us\\b', word):\n",
    "        new_words = []\n",
    "        for term in word.split('/'):\n",
    "            characters = set(\"!#$%&'()*+^,\")\n",
    "            new_word = term.translate({ord(char) : None for char in characters})\n",
    "            delete_chars = \"+_\\-\\.\"\n",
    "            new_word = re.sub(r'^[{}]+|[{}]+$'.format(delete_chars, delete_chars), '', new_word)\n",
    "            new_word = re.sub(r'^¿([^\\W_])', r'\\1', new_word) #deletes '¿' at the beginning of the word\n",
    "            if new_word:\n",
    "                new_words.append(new_word)\n",
    "            \n",
    "        return new_words\n",
    "    return [re.sub(r'^[/\\'\\+]+|/$', '', word)]\n",
    "\n",
    "def tokenize(doc, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        words = [word.strip() for word in casual_tokenize(doc) if word.strip() not in stoplist]\n",
    "    else:\n",
    "        words = [word for word in word_tokenize(doc)]\n",
    "    words = [token for word in words for token in remove_punctuation(word)]\n",
    "    words = [word for word in words if len(word) > 1 and not is_number(word) and is_english(word)]\n",
    "    words = [text_from_number(word).groups()[0] if text_from_number(word) else word for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run Preprocess notebook first!\n",
    "question_pairs = pickle.load(open('data/question_pairs.list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading questions and building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read unique questions from pairs\n",
    "def process_questions(output_filename):\n",
    "    questions = []\n",
    "    seen_questions = set() #question to question id, to correct question ids (check corrected data file)\n",
    "    vocabulary = set()\n",
    "\n",
    "    with open(output_filename, 'w') as qt_file:\n",
    "        for pair in question_pairs:\n",
    "            question1, question2 = pair[3:5]\n",
    "\n",
    "            token_q1 = tokenize(question1)\n",
    "            token_q2 = tokenize(question2)\n",
    "\n",
    "            qt_file.write('{}\\n{}\\n{}\\n{}\\n'.format(question1, ' '.join(token_q1),\n",
    "                                                    question2, ' '.join(token_q2)))\n",
    "\n",
    "            if question1 not in seen_questions:\n",
    "                seen_questions.add(question1)\n",
    "                vocabulary.update(token_q1)\n",
    "                if token_q1:\n",
    "                    questions.append(token_q1)\n",
    "\n",
    "            if question2 not in seen_questions:\n",
    "                seen_questions.add(question2)\n",
    "                vocabulary.update(token_q2)\n",
    "                if token_q2:\n",
    "                    questions.append(token_q2)\n",
    "                    \n",
    "    return questions, vocabulary\n",
    "\n",
    "%time questions, vocabulary = process_questions('data/questions_tokenized.txt')\n",
    "\n",
    "pkl_file = open('data/lsa_vocabulary.set.pkl', 'wb')\n",
    "pickle.dump(vocabulary, pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vocabulary.txt','w') as outfile:\n",
    "    for word in vocabulary:\n",
    "        outfile.write('{}\\n'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('questions:', len(questions))\n",
    "print('vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(questions)\n",
    "dictionary.save('data/questions.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsvectors = [dictionary.doc2bow(document) for document in questions]\n",
    "gensim.corpora.MmCorpus.serialize('data/questions.mm', questionsvectors)\n",
    "print(questionsvectors[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"how can I stay motivated to learn a new language\"\n",
    "new_vec = dictionary.doc2bow(new_doc.split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files\n",
    "\n",
    "If saved files previously, they can be read from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('data/questions.dict')\n",
    "corpus = gensim.corpora.MmCorpus('data/questions.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.save('data/questions.tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "lsa.save('data/questions.10d.lsa')\n",
    "%time lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
    "lsa.save('data/questions.100d.lsa')\n",
    "%time lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
    "lsa.save('data/questions.200d.lsa')\n",
    "%time lsa = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "lsa.save('data/questions.300d.lsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel.load('data/questions.tfidf')\n",
    "lsa = gensim.models.LsiModel.load('data/questions.10d.lsa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('data/questions.dict')\n",
    "lsa10 = gensim.models.LsiModel.load('data/questions.10d.lsa')\n",
    "doc1 = \"what is npt? how would signing the npt affect india?\"\n",
    "doc2 = \"what is npt?\"\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "token_doc1 = tokenize(doc1)\n",
    "token_doc2 = tokenize(doc2)\n",
    "print(token_doc1)\n",
    "print(token_doc2)\n",
    "doc1_vec = lsa10[dictionary.doc2bow(token_doc1)]\n",
    "doc2_vec = lsa10[dictionary.doc2bow(token_doc2)]\n",
    "print(doc1_vec)\n",
    "print(doc2_vec)\n",
    "print(gensim.matutils.cossim(doc1_vec, doc2_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using cosine similarity a file is generated to be evaluated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "#just to debug time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ndim in [10, 100, 200, 300]:\n",
    "    print('Evaluating {} dimensions'.format(ndim))\n",
    "    filename = 'data/lsa_similarities_{}d.txt'.format(ndim)\n",
    "    print('{} - Loading model...'.format(strftime('%H:%M:%S')))\n",
    "    lsa = gensim.models.LsiModel.load('data/questions.{}d.lsa'.format(ndim))\n",
    "    outfile = open(filename, 'w')\n",
    "    errfile = None\n",
    "    print('{} - Starting evaluation...'.format(strftime('%H:%M:%S')))\n",
    "    \n",
    "    for _, pair in enumerate(question_pairs):\n",
    "        pair_id, qid1, qid2, doc1, doc2, label = pair\n",
    "        token_doc1 = tokenize(doc1)\n",
    "        token_doc2 = tokenize(doc2)\n",
    "        \n",
    "        doc1_vec = lsa[dictionary.doc2bow(token_doc1)]\n",
    "        doc2_vec = lsa[dictionary.doc2bow(token_doc2)]\n",
    "        similarity = gensim.matutils.cossim(doc1_vec, doc2_vec)\n",
    "        \n",
    "        outfile.write('{}|{}|{}\\n'.format(pair_id, label, similarity))\n",
    "        \n",
    "    print('{} - Evaluation finished.'.format(strftime('%H:%M:%S')))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('data/lsa_results.txt', 'w')\n",
    "for ndim in [10, 100, 200, 300]:\n",
    "    filename = 'data/lsa_similarities_{}d.txt'.format(ndim)\n",
    "    values_file = open('data/lsa_{}d_values.txt'.format(ndim), 'w')\n",
    "\n",
    "    with open(filename) as infile:\n",
    "        tp = fp = tn = fn = 0\n",
    "        for line in infile:\n",
    "            pair_id, label, similarity = line.split('|')\n",
    "            pair_id = int(pair_id)\n",
    "            label = int(label)\n",
    "            similarity = float(similarity)\n",
    "\n",
    "            predicted_label = 0 if similarity < 0.7 else 1\n",
    "\n",
    "            if label == 1: #positive\n",
    "                if label == predicted_label: #true positive\n",
    "                    tp += 1\n",
    "                else: #false negative\n",
    "                    fn += 1\n",
    "            else: #negatives\n",
    "                if label == predicted_label: #true negative\n",
    "                    tn += 1\n",
    "                else: #false positive\n",
    "                    fp += 1\n",
    "            q1 = question_pairs[pair_id][3]\n",
    "            q2 = question_pairs[pair_id][4]\n",
    "            values_file.write('{} - {} - {} - {}\\n{}\\n{}\\n==================\\n'.format(pair_id, label, predicted_label, \\\n",
    "                                                                                     similarity, q1, q2))\n",
    "\n",
    "        wstr = '\\nLSA - {} dimensions\\n'.format(ndim) +\\\n",
    "               '\\t\\t\\tpredicted_no\\t\\tpredicted_yes\\n'+\\\n",
    "               'actual_no\\t\\t    {}\\t\\t    {}\\n'.format(tn, fp)+\\\n",
    "               'actual_yes\\t\\t    {}\\t\\t    {}\\n\\n'.format(fn, tp)\n",
    "        outfile.write(wstr)\n",
    "        accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "        precision = tp/(tp + fp)\n",
    "        recall = tp/(tp + fn)\n",
    "\n",
    "        outfile.write('accuracy: {0:.3f}\\nprecision: {0:.3f}\\nrecall: {0:.3f}\\n'.format(accuracy, precision, recall))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
