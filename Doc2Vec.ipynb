{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(stopwords.words('english'))\n",
    "def is_english(word):\n",
    "    #not quite, it accepts some latin characters\n",
    "    try:\n",
    "        word.encode('ISO-8859-1')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_number(word):\n",
    "    #plus hexadecimal numbers and 1 letter + numbers\n",
    "    return re.search(r'(^0x)|(^[0-9\\-\\.:xv]+$)|(^\\w\\d+$)', word)\n",
    "\n",
    "def text_from_number(word):\n",
    "    return re.search(r'^\\d+([a-z]+)$', word)\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    if not re.search(r'\\.com?\\b|\\.in\\b|\\.org\\b|\\.be\\b|\\.xyz\\b|\\.net\\b|\\.us\\b', word):\n",
    "        new_words = []\n",
    "        for term in word.split('/'):\n",
    "            characters = set(\"!#$%&'()*+^,\")\n",
    "            new_word = term.translate({ord(char) : None for char in characters})\n",
    "            delete_chars = \"+_\\-\\.\"\n",
    "            new_word = re.sub(r'^[{}]+|[{}]+$'.format(delete_chars, delete_chars), '', new_word)\n",
    "            new_word = re.sub(r'^Â¿([^\\W_])', r'\\1', new_word) #deletes 'Â¿' at the beginning of the word\n",
    "            if new_word:\n",
    "                new_words.append(new_word)\n",
    "            \n",
    "        return new_words\n",
    "    return [re.sub(r'^[/\\'\\+]+|/$', '', word)]\n",
    "\n",
    "def tokenize(doc, remove_stopwords=True):\n",
    "    if remove_stopwords:\n",
    "        words = [word.strip() for word in casual_tokenize(doc) if word.strip() not in stoplist]\n",
    "    else:\n",
    "        words = [word for word in word_tokenize(doc)]\n",
    "    words = [token for word in words for token in remove_punctuation(word)]\n",
    "    words = [word for word in words if len(word) > 1 and not is_number(word) and is_english(word)]\n",
    "    words = [text_from_number(word).groups()[0] if text_from_number(word) else word for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs = pickle.load(open('data/question_pairs.list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pickle file\n",
    "\n",
    "Execute if files need to be generated, otherwise, just load the pickles ðŸ¥’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "qids = set()\n",
    "for pair in question_pairs:\n",
    "    qid1, qid2, question1, question2 = pair[1:5]\n",
    "\n",
    "    if qid1 not in qids:\n",
    "        qids.add(qid1)\n",
    "        questions.append((question1, qid1))\n",
    "\n",
    "    if qid2 not in qids:\n",
    "        qids.add(qid2)\n",
    "        questions.append((question2, qid2))\n",
    "\n",
    "#should this be saved?\n",
    "outfile = open('data/question_ids.list.pkl','wb')\n",
    "pickle.dump(questions, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip this if you generated the pickle file in the cell before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pickle.load(open('data/question_ids.list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tagged documents for the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [gensim.models.doc2vec.TaggedDocument(tokenize(question), [qid]) for question, qid in questions]\n",
    "picfile = open('data/d2v_tagged_documents.list.pkl', 'wb')\n",
    "pickle.dump(documents, picfile)\n",
    "picfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(gensim.models.doc2vec.FAST_VERSION)\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no hierarchical softmax\n",
    "#no negative sampling\n",
    "#cores = multiprocessing.cpu_count() #for some reason 1 worker is faster\n",
    "model = gensim.models.Doc2Vec(vector_size=100, window=2, hs=0, min_count=2, workers=1, epochs=50)\n",
    "model.build_vocab(documents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.vocab))\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model2.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "#just to log time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train doc2vec with all the corpus\n",
    "#cores = multiprocessing.cpu_count()\n",
    "cores = 1\n",
    "word_window = 2\n",
    "min_count = 2 #ignore words with less than this frequency\n",
    "epochs = 100 #training cycles\n",
    "hierarchical_softmax = 0\n",
    "sampling = 0.00001 #1e-5\n",
    "ns = 5 #number of negative sample (5-20), 0: not used\n",
    "\n",
    "for ndim in [10, 100, 200, 300]:\n",
    "    print('{} - model with {} dimensions'.format(strftime('%H:%M:%S'), ndim))\n",
    "    model = gensim.models.Doc2Vec(vector_size=ndim, window=word_window, hs=0, sample=sampling, negative=ns,\n",
    "                                  min_count=min_count, workers=cores)\n",
    "    print('{} - Building vocabulary...'.format(strftime('%H:%M:%S')))\n",
    "    model.build_vocab(documents)\n",
    "    print('{} - Training model...'.format(strftime('%H:%M:%S')))\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=epochs)\n",
    "    print('{} - Done.\\n'.format(strftime('%H:%M:%S')))\n",
    "    model.save('data/questions.{}d.d2v'.format(ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Remember to initialize tokenizer at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this comes for the preprocessing ipython notebook\n",
    "question_pairs = pickle.load(open('data/question_pairs.list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, acos, pi\n",
    "def cosine_similarity(v1, v2):\n",
    "    #numpy arrays\n",
    "    vec_sum = (v1*v2).sum()\n",
    "    v1_sum = (v1**2).sum()\n",
    "    v2_sum = (v2**2).sum()\n",
    "    \n",
    "    similarity = vec_sum/(sqrt(v1_sum)*sqrt(v2_sum))\n",
    "    \n",
    "    if similarity > 1:\n",
    "        similarity = 1\n",
    "\n",
    "    return 1 - (acos(similarity)/pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ndim in [10, 100, 200, 300]:\n",
    "    print('Evaluating {} dimensions'.format(ndim))\n",
    "    outfile = open('data/d2v_similarities_{}d.txt'.format(ndim), 'w')\n",
    "    print('{} - Loading model...'.format(strftime('%H:%M:%S')))\n",
    "    model = gensim.models.Doc2Vec.load('data/questions.{}d.d2v'.format(ndim))\n",
    "    print('{} - Starting evaluation...'.format(strftime('%H:%M:%S')))\n",
    "    for line in question_pairs:\n",
    "        pair_id = line[0]\n",
    "        question1, question2, label = line[3:6]\n",
    "\n",
    "        tokenized_question1 = tokenize(question1)\n",
    "        tokenized_question2 = tokenize(question2)\n",
    "\n",
    "        vec_q1 = model.infer_vector(tokenized_question1)\n",
    "        vec_q2 = model.infer_vector(tokenized_question2)\n",
    "\n",
    "        similarity = cosine_similarity(vec_q1, vec_q2)\n",
    "\n",
    "        outfile.write(\"{}|{}|{}\\n\".format(pair_id, label, similarity))\n",
    "        \n",
    "    print('{} - Evaluation finished.'.format(strftime('%H:%M:%S')))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('data/d2v_results.txt', 'w')\n",
    "for ndim in [10, 100, 200, 300]:\n",
    "    filename = 'data/d2v_similarities_{}d.txt'.format(ndim)\n",
    "    values_file = open('data/d2v_{}d_values.txt'.format(ndim), 'w')\n",
    "\n",
    "    with open(filename) as infile:\n",
    "        tp = fp = tn = fn = 0\n",
    "        for line in infile:\n",
    "            pair_id, label, similarity = line.split('|')\n",
    "            pair_id = int(pair_id)\n",
    "            label = int(label)\n",
    "            similarity = float(similarity)\n",
    "\n",
    "            predicted_label = 0 if similarity < 0.7 else 1\n",
    "\n",
    "            if label == 1: #positive\n",
    "                if label == predicted_label: #true positive\n",
    "                    tp += 1\n",
    "                else: #false negative\n",
    "                    fn += 1\n",
    "            else: #negatives\n",
    "                if label == predicted_label: #true negative\n",
    "                    tn += 1\n",
    "                else: #false positive\n",
    "                    fp += 1\n",
    "            q1 = question_pairs[pair_id][3]\n",
    "            q2 = question_pairs[pair_id][4]\n",
    "            values_file.write('{} - {} - {} - {}\\n{}\\n{}\\n==================\\n'.format(pair_id, label, predicted_label, \\\n",
    "                                                                                     similarity, q1, q2))\n",
    "\n",
    "        wstr = '\\nD2V - {} dimensions\\n'.format(ndim) +\\\n",
    "               '\\t\\t\\tpredicted_no\\t\\tpredicted_yes\\n'+\\\n",
    "               'actual_no\\t\\t    {}\\t\\t    {}\\n'.format(tn, fp)+\\\n",
    "               'actual_yes\\t\\t    {}\\t\\t    {}\\n\\n'.format(fn, tp)\n",
    "        outfile.write(wstr)\n",
    "        accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "        precision = tp/(tp + fp)\n",
    "        recall = tp/(tp + fn)\n",
    "\n",
    "        outfile.write('accuracy: {0:.3f}\\nprecision: {0:.3f}\\nrecall: {0:.3f}\\n'.format(accuracy, precision, recall))\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
